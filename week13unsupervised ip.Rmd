---
title: "week13-unsupervised learning"
author: "Mercy Cherotich"
date: "1/29/2022"
output: html_document
---


**RESEARCH QUESTION**

Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand's Sales and Marketing team would like to understand their customer's behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.

**PROBLEM STATEMENT**

Most website business attracts many visitors. Anyone who land on a business website has some sort of interest in what one has to offer. However, not everyone would visit a site to buy. Some users might be interested in only gaining the information, and not making a purchase. The result will be a loss lead and sales if the conversion rates are low.
The objective of this analysis therefore is to provide a reliable and feasible recommendation algorithm to understand their customer's behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.. The target value is the binary 'FALSE' or 'TRUE' regarding the website visitors' decision to buy. The plan is to use clustering a classification techniques to be able to make predictions about shoppers' intentions.

**DATA**

The dataset we are using consists of 10 numerical and 8 categorical attributes.

**Administrative, Administrative Duration, Informational, Informational Duration, Product Related and Product Related Duration** represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another.

The **Bounce Rate, Exit Rate and Page Value** features represent the metrics measured by "Google Analytics" for each page in the e-commerce site.

**Bounce Rate** - feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session. This is the number of single-page visits by visitors of the website.

**Exit Rate** - feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. This is the number of exits from the website.

**Page Value** - feature represents the average value for a web page that a user visited before completing an e-commerce transaction. It tells you which specific pages of the site offer the most value. For instance, a product page for an Ecommerce site will usually have a higher page value than a resource page.

**Special Day** - feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother's Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina's day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.

The dataset also includes **operating system, browser, region, traffic type, visitor type** as *returning*,*other* or *new visitor*, a Boolean value indicating whether the date of the visit is weekend, and month of the year.


**Revenue** - has the client purchased a product on the website? (binary: 'TRUE', 'FALSE')
# Reading the data
**loading libraries**
```{r,echo=FALSE}
#install.packages("readxl")
library("readxl")
require(dplyr)
#install.packages("moments")
library(moments)
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
#install.packages("superml")
library("superml")
#install.packages('tidyverse')
library(tidyverse)

#install.packages('ggplot2')
library(ggplot2)
library(caret)
```



````{r}

# csv files
my_data <- read.csv("C:\\Users\\MERCY\\Downloads\\online_shoppers_intention (1).csv")
```

# Checking the data

```{r}
#previewing the head
head(my_data)

```

````{r}
#previewing the tail
tail(my_data)
```

````{r}
#checking size and data types
str(my_data)
#our data has as 12330 rows and 18 variables
#14 of the  variables are numeric ,2 are characters and 2 are boolen
```


````{r}
#checking for unique values in the colums
a<-unique((my_data$Month))
sapply(my_data,n_distinct)
```


# Data cleaning
````{r}
#checking for the null values
colSums(is.na(my_data))
#they are few missing values hence we drop
data<-na.omit(my_data)
colSums((is.na(data)))
```



````{r}
#checking duplicates
duplicates <- data[duplicated(data),]
duplicates
#looking at the duplicates there is no need to drop the contain different infomation
```
````{r}
#checking for outliers
str(data)
#sublist of the numeric columns
data_numeric<- data[,c(1:9)]
head(data_numeric)
#plotting box plot
boxplot(data_numeric$Administrative,xlab="Administrative")
boxplot(data_numeric$Administrative_Duration,xlab="admin_duration")
boxplot(data_numeric$Informational,x_lab="informationa")
boxplot(data_numeric$Informational_Duration,xlab="info_duration")
boxplot(data_numeric$ProductRelated,xlab="prod_related")
boxplot(data_numeric$ProductRelated_Duration,xlab="prod_duration")
boxplot(data_numeric$BounceRates,xlab="bounce rates")
boxplot(data_numeric$ExitRates,xlab="exit rates")
boxplot(data_numeric$PageValues,xlab="pages values")
  #our data contain outliers but this could be because of the difference but we wont drop them as this could be source of variation in our data and thus having important info7

```
# Univariate Analysis
**those who generated revenue**
````{r}
#spliting the data based on revenue since it is the label data set
data_true <- filter(data,Revenue == "TRUE")
summary(data_true)
```
the mean bounce rate for the once who generate revenue is 0.005,exit rate is 0.01 and page values is 27.26.they mostly did productrelated service followed by administrative and finally the informational

````{r}
data_T <- data_true[,c(1:10,12:15)]
desc_stats <- data.frame(
  skewness= apply(data_T, 2, skewness),    
  kurtosis = apply(data_T, 2, kurtosis), 
  SD = apply(data_T, 2, sd),      
  Max = apply(data_T, 2, max),     
  var = apply(data_T,2,var),
  mode = apply(data_T,2,getmode)
  
)
desc_stats <- round(desc_stats, 1)
desc_stats


```


````{r}
browser<- data_true$Browser
browser<- table(browser)
browser
#visualising
barplot(browser)
```
the clients who generated income mostly used browser two

````{r}
traffic<- data_true$TrafficType
traffic<-table(traffic)
traffic


```
the most used traffic type is 2
````{r}
region<- data_true$Region
region<- table(region)
region
#visualising the regions
barplot(region,col=c("darkblue","red"),)
```
the most clients are from region one
````{r}

visitors <- data_true$VisitorType
visitors<- table(visitors)
#visualising the visitors
barplot(visitors,col=c("pink"))
```
.the most client who generated revenue are the returning visitors who visits and come later
````{r}
o.s <- data_true$OperatingSystems
o.s<-table(o.s)
#visualising the barplot
barplot(o.s,col=c("red"))
```
the most preffered os was 2 by client who generated revenue
````{r}

weekend <- data_true$Weekend
weekend<- table(weekend)
weekend
#visualising the weekedn tabke
barplot(weekend,col=c("blue","pink"))

```


most revenue was generated in weekdays

**those who did not generated revenue**
````{r}
#spliting the data based on revenue since it is the label data set
data_false <- filter(data,Revenue == "FALSE")
summary(data_false)
```
 mean bounce rate for the once who generate revenue is 0.02,exit rate is 0.04 and page values is 1.78.they mostly did productrelated service followed by administrative and finally the informational


````{r}
data_F <- data_false[,c(1:10,12:15)]
desc_stats <- data.frame(
  skewness= apply(data_F, 2, skewness),    
  kurtosis = apply(data_F, 2, kurtosis), 
  SD = apply(data_F, 2, sd),      
  Max = apply(data_F, 2, max),     
  var = apply(data_F,2,var),
  mode = apply(data_F,2,getmode)
  
)
desc_stats <- round(desc_stats, 1)
desc_stats


```


````{r}
browser<- data_false$Browser
browser<- table(browser)
browser
#visualising
barplot(browser,col=c("yellow"))
```
the most used browser by those who did nit generate revenue is 2
````{r}
traffic<- data_false$TrafficType
traffic<-table(traffic)
traffic


```
the most preferred trafic by those who donot generate ibcome is 2


````{r}
region<- data_false$Region
region<- table(region)
region
#visualising the regions
barplot(region,col=c("darkblue","red"),)
```
most clients who did not generate income are from region 1

````{r}

visitors <- data_false$VisitorType
visitors<- table(visitors)
#visualising the visitors
barplot(visitors,col=c("pink"))
```
most visitors who did not generate reenue are the returning visitors
````{r}
o.s <- data_false$OperatingSystems
o.s<-table(o.s)
#visualising the barplot
barplot(o.s)

```
most preferred os is type 2

````{r}

weekend <- data_false$Weekend
weekend<- table(weekend)
weekend
#visualising the weekedn tabke
barplot(weekend,col=c("blue","pink"))
```
# Bivariate Analysis

````{r}


#relationshiop between bounce rate and ad exit rate
plot(data$ExitRates,data$BounceRates)
cor(data$ExitRates,data$BounceRates)
```
the exit rates and bounce rate has strongh positive correlation since as one increase he other one also increases

**Revenue and weekend**
````{r}
table(data$Revenue,data$Month)
#visualising the revenue and month
revenue <- with(data, table(Revenue , Month))
barplot(revenue,legend=TRUE, beside = TRUE,col=c("green","grey"),
        main = "Revenue by Month",xlab = "Months")


```
month of nov had the leading revenue and may was the leading month when revenue was not genarated

````{r}
#relationship between revenue and operating system

table(data$Revenue,data$OperatingSystems)
#visualising the revenue and month
revenue <- with(data, table(Revenue ,OperatingSystems ))
barplot(revenue,legend=TRUE, beside = TRUE,col=c("blue","pink"),
        main = "Revenue by OperatingSystems",xlab = "OperatingSystems")
```
os 2 was the one mostly used

````{r}
#relationship between revenue and visitors type

table(data$Revenue,data$VisitorType)
#visualising the revenue and month
revenue <- with(data, table(Revenue ,VisitorType ))
barplot(revenue,legend=TRUE, beside = TRUE,col=c("blue","red"),
        main = "Revenue by visitorstype",xlab = "visitortype")
```
the leading type of customers were the returnung ones

````{r}
#relationship between revenue and broiwser

table(data$Revenue,data$Browser)
#visualising the revenue and month
revenue <- with(data, table(Revenue ,Browser ))
barplot(revenue,legend=TRUE, beside = TRUE,col=c("grey","pink"),
        main = "Revenue by browser",xlab = "browser")
```
the most prefered browser was 2

````{r}
#relationship between revenue and region

table(data$Revenue,data$Region)
#visualising the revenue and month
region <- with(data, table(Revenue ,Region ))
barplot(region,legend=TRUE, beside = TRUE,col=c("blue","pink"),
        main = "Revenue by Region",xlab = "Region")
```
refion one had the leading clients

````{r}
#relationship between revenue and traffic

traffic<-table(data$Revenue,data$TrafficType)
traffic

  

```
the leading traffic was 2


````{r}
#converting the categorical data into numeric
library(caret)

dmy <- dummyVars(" ~ .", data = data, fullRank = T)
data_df <- data.frame(predict(dmy, newdata = data))

head(data_df)

```

# Data processing
**dropping revenue column**
````{r}
#we drop the revenue column since it is the label
df<- data_df[,-c(27)]
head(df)
```
**data scaling**
````{r}
df<- scale(df)
head(df)

```
# Kmeans
K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarityr

- 
```{r}
#determine number of clusters using elbow method
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(df, centers = i, nstart = 25)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}
#plot a scree plot

plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")


#from the screeplot we can see the optimum number of clusters is 12 since that is where the elbow shape formed
#but since we already have the number of clusters in the label column we are are going to use k as 2
```
````{r}
#performing clustering with the optimal number of clusters
k_means <- kmeans(df, 2,nstart = 25)
# Checking the cluster centers of each variable
k_means$centers
```

````{r}
# Previewing the size of observations in each cluster
k_means$size
```


````{r}
## Our cluster centers (means)
k_means$centers
```

````{r}
## Between cluster sum of squares
k_means$betweenss
```

````{r}
## Total cluster sum of squares
k_means$totss
```


````{r}
#comparing the results of k_means and the real clusters from the revenue column
comp <- table(k_means$cluster)
comp
comp2<- table(data$Revenue)
comp2
```

# Hierachial clustering

```{r}
# Distance matrix using euclidean
d <- dist(df,method = "euclidean")
# Hierarchical clustering using Complete Linkage
res.cls<- hclust(d, method = "complete")
res.cls
```


```{r}
# Plot the obtained dendrogram
plot(res.cls, cex = 0.6, hang = -1)
```


```{r}
#using cut tree to get the two clusters
cutree(res.cls,k=2)
```



##COMPARISON

##KMEANS CLUSTERING
K-means clustering is a machine learning clustering technique used to simplify large datasets into smaller and simple datasets. Distinct patterns are evaluated and similar data sets are grouped together. The variable K represents the number of groups in the data. This article evaluates the pros and cons of the K-means clustering algorithm to help you weigh the benefits of using this clustering technique.

 

**Pros**
1. Simple: It is easy to implement k-means and identify unknown groups of data from complex data sets. The results are presented in an easy and simple manner.

2. Flexible: K-means algorithm can easily adjust to the changes. If there are any problems, adjusting the cluster segment will allow changes to easily occur on the algorithm.
3. Suitable in a large dataset: K-means is suitable for a large number of datasets and it's computed much faster than the smaller dataset. It can also produce higher clusters.

4. Efficient: The algorithm used is good at segmenting the large data set. Its efficiency depends on the shape of the clusters. K-means works well in hyper-spherical clusters.

5. Time complexity: K-means segmentation is linear in the number of data objects thus increasing execution time. It doesn't take more time in classifying similar characteristics in data like hierarchical algorithms.

6. Easy to interpret: The results are easy to interpret. It generates cluster descriptions in a form minimized to ease understanding of the data.

7. Computation cost: Compared to using other clustering methods, a k-means clustering technique is fast and efficient in terms of its computational cost O(K*n*d).

8. Accuracy: K-means analysis improves clustering accuracy and ensures information about a particular problem domain is available. Modification of the k-means algorithm based on this information improves the accuracy of the clusters.



 

**Cons**

1. NoNo-optimal set of clusters: K-means doesn't allow the development of an optimal set of clusters and for effective results, you should decide on the clusters before.

2. Lacks consistency: K-means clustering gives varying results on different runs of an algorithm. A random choice of cluster patterns yields different clustering results resulting in inconsistency.

3. Uniform effect: It produces clusters with uniform sizes even when the input data has different sizes.
4. Order of values: The way in which data is ordered in building the algorithm affects the final results of the data set.

5. Sensitivity to scale: Changing or rescaling the dataset either through normalization or standardization will completely change the final results.

6. Crash computer: When dealing with a large dataset, conducting a dendrogram technique will crash the computer due to a lot of computational load and Ram limits.
7. Handle numerical data: K-means algorithm can be performed in numerical data only.



##HIERARCHICAL CLUSTERING

Hierarchical clustering works  by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: 
(1) identify the two clusters that are closest together, and 
(2) merge the two most similar clusters. 
This iterative process continues until all the clusters are merged together.

##Pros:##

 -We do not need to specify the number of clusters required for the algorithm.
 -Hierarchical clustering outputs a hierarchy, ie a structure that is more informative than the unstructured set of flat clusters returned by k-means.
 -It is also easy to implement.

**Cons

 -There is no mathematical objective for Hierarchical clustering.
 -All the approaches to calculate the similarity between clusters has its own disadvantages.
 -High space and time complexity for Hierarchical clustering. Hence this clustering algorithm cannot be used when we have huge data.


















